{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "STROKE_COUNT = 196\n",
    "TRAIN_SAMPLES = 10000\n",
    "VALID_SAMPLES = 750\n",
    "TEST_SAMPLES = 750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "def top_3_accuracy(x,y): return top_k_categorical_accuracy(x,y, 3)\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from glob import glob\n",
    "import gc\n",
    "gc.enable()\n",
    "def get_available_gpus():\n",
    "    from tensorflow.python.client import device_lib\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "base_dir = os.path.join('/mnt', 'quickdraw')\n",
    "test_path = os.path.join(base_dir, 'test_simplified.csv')\n",
    "if len(get_available_gpus())>0:\n",
    "    # https://twitter.com/fchollet/status/918170264608817152?lang=en\n",
    "    print(\"Using GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "ALL_TRAIN_PATHS = glob(os.path.join(base_dir, 'train_simplified', '*.csv'))\n",
    "COL_NAMES = ['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word']\n",
    "\n",
    "def _stack_it(raw_strokes):\n",
    "    \"\"\"preprocess the string and make \n",
    "    a standard Nx3 stroke vector\"\"\"\n",
    "    stroke_vec = literal_eval(raw_strokes) # string->list\n",
    "    # unwrap the list\n",
    "    in_strokes = [(xi,yi,i)  \n",
    "     for i,(x,y) in enumerate(stroke_vec) \n",
    "     for xi,yi in zip(x,y)]\n",
    "    c_strokes = np.stack(in_strokes)\n",
    "    # replace stroke id with 1 for continue, 2 for new\n",
    "    c_strokes[:,2] = [1]+np.diff(c_strokes[:,2]).tolist()\n",
    "    c_strokes[:,2] += 1 # since 0 is no stroke\n",
    "    # pad the strokes with zeros\n",
    "    # print(c_strokes)\n",
    "    return pad_sequences(c_strokes.swapaxes(0, 1), \n",
    "                         maxlen=STROKE_COUNT, \n",
    "                         padding='post').swapaxes(0, 1)\n",
    "def read_batch(samples=5, start_row = 0):\n",
    "    \"\"\"\n",
    "    load and process the csv files\n",
    "    this function is horribly inefficient but simple\n",
    "    \"\"\"\n",
    "    out_df_list = []\n",
    "    for c_path in ALL_TRAIN_PATHS:\n",
    "        c_df = pd.read_csv(c_path, nrows=samples, skiprows=start_row)\n",
    "        #print(type(c_df), \";\")\n",
    "        c_df.columns=COL_NAMES\n",
    "        out_df_list += [c_df[['drawing', 'word']]]\n",
    "        #print(out_df_list, \"==\")\n",
    "    full_df = pd.concat(out_df_list)\n",
    "    full_df['drawing'] = full_df['drawing'].\\\n",
    "        map(_stack_it)\n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set is loading\n"
     ]
    }
   ],
   "source": [
    "train_args = dict(samples=TRAIN_SAMPLES, \n",
    "                  start_row=0)\n",
    "valid_args = dict(samples=VALID_SAMPLES, \n",
    "                  start_row=train_args['samples']+1)\n",
    "test_args = dict(samples=TEST_SAMPLES, \n",
    "                 start_row=valid_args['samples']+train_args['samples']+1)\n",
    "print(\"training set is loading\")\n",
    "train_df = read_batch(**train_args)\n",
    "print(\"training set is loaded\")\n",
    "valid_df = read_batch(**valid_args)\n",
    "print(\"dev set is loaded\")\n",
    "test_df = read_batch(**test_args)\n",
    "print(\"test set is loaded\")\n",
    "word_encoder = LabelEncoder()\n",
    "word_encoder.fit(train_df['word'])\n",
    "print('words', len(word_encoder.classes_), '=>', ', '.join([x for x in word_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Xy(in_df):\n",
    "    X = np.stack(in_df['drawing'], 0)\n",
    "    y = to_categorical(word_encoder.transform(in_df['word'].values))\n",
    "    return X, y\n",
    "train_X, train_y = get_Xy(train_df)\n",
    "valid_X, valid_y = get_Xy(valid_df)\n",
    "test_X, test_y = get_Xy(test_df)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, m_axs = plt.subplots(3,3, figsize = (16, 16))\n",
    "rand_idxs = np.random.choice(range(train_X.shape[0]), size = 9)\n",
    "for c_id, c_ax in zip(rand_idxs, m_axs.flatten()):\n",
    "    test_arr = train_X[c_id]\n",
    "    test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n",
    "    lab_idx = np.cumsum(test_arr[:,2]-1)\n",
    "    for i in np.unique(lab_idx):\n",
    "        c_ax.plot(test_arr[lab_idx==i,0], \n",
    "                np.max(\n",
    "                    test_arr[:,1])-test_arr[lab_idx==i,1], '.-')\n",
    "    c_ax.axis('off')\n",
    "    c_ax.set_title(word_encoder.classes_[np.argmax(train_y[c_id])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updatable plot\n",
    "# a minimal example (sort of)\n",
    "import keras\n",
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()\n",
    "\n",
    "class NBatchLogger(keras.callbacks.Callback):\n",
    "    def __init__(self,display=100):\n",
    "        '''\n",
    "        display: Number of batches to wait before outputting loss\n",
    "        '''\n",
    "        self.seen = 0\n",
    "        self.display = display\n",
    "\n",
    "    def on_batch_end(self,batch,logs={}):\n",
    "        self.seen += logs.get('size', 0)\n",
    "        if self.seen % self.display == 0:\n",
    "            #print(self.seen)\n",
    "            print('')\n",
    "#            print('{0} {1}'.format(self.params['metrics'][0], self.params['metrics'][1]))\n",
    "            \n",
    "out_batch = NBatchLogger(display=10)\n",
    "\n",
    "def apk(actual, predicted, k=3):\n",
    "\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    return K.mean([apk(a,p,k) for a,p in zip(actual, predicted)])\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    print(y_true)\n",
    "    print(y_pred)\n",
    "    return mapk(K.eval(y_true), K.eval(y_pred))\n",
    "\n",
    "class Metrics(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self._data = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        X_val, y_val = self.validation_data[0], self.validation_data[1]\n",
    "        y_predict = np.asarray(self.model.predict(X_val))\n",
    "\n",
    "        y_val = np.argmax(y_val, axis=1)\n",
    "        y_predict = np.argmax(y_predict, axis=1)\n",
    "\n",
    "        self._data.append({\n",
    "            'mapk3': mapk(y_val, y_predict),\n",
    "        })\n",
    "        return\n",
    "\n",
    "    def get_data(self):\n",
    "        return self._data\n",
    "metrics = Metrics()\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Conv1D, LSTM, Dense, Dropout\n",
    "if len(get_available_gpus())>0:\n",
    "    # https://twitter.com/fchollet/status/918170264608817152?lang=en\n",
    "    print(\"GPU\")\n",
    "    from keras.layers import CuDNNLSTM as LSTM # this one is about 3x faster on GPU instances\n",
    "stroke_read_model = Sequential()\n",
    "stroke_read_model.add(BatchNormalization(input_shape = (None,)+train_X.shape[2:]))\n",
    "# filter count and length are taken from the script https://github.com/tensorflow/models/blob/master/tutorials/rnn/quickdraw/train_model.py\n",
    "stroke_read_model.add(Conv1D(48, (5,)))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(Conv1D(64, (5,)))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(Conv1D(96, (3,)))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(LSTM(128, return_sequences = True))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(LSTM(128, return_sequences = False))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(Dense(512))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(Dense(len(word_encoder.classes_), activation = 'softmax'))\n",
    "stroke_read_model.compile(optimizer = 'adam', \n",
    "                          loss = 'categorical_crossentropy', \n",
    "                          metrics = ['categorical_accuracy', top_3_accuracy])\n",
    "stroke_read_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path=\"{}_weights.best.hdf5\".format('stroke_lstm_model')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, \n",
    "                                   verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=5) # probably needs to be more patient, but kaggle time is limited\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat, out_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "history = stroke_read_model.fit(train_X, train_y,\n",
    "                      validation_data = (valid_X, valid_y), \n",
    "                      batch_size = batch_size,\n",
    "                      epochs = 20,\n",
    "                      callbacks = callbacks_list)\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_read_model.load_weights(weight_path)\n",
    "lstm_results = stroke_read_model.evaluate(test_X, test_y, batch_size = batch_size)\n",
    "print('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%' % (100*lstm_results[1], 100*lstm_results[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "test_cat = np.argmax(test_y, 1)\n",
    "pred_y = stroke_read_model.predict(test_X, batch_size = batch_size)\n",
    "pred_cat = np.argmax(pred_y, 1)\n",
    "plt.matshow(confusion_matrix(test_cat, pred_cat))\n",
    "print(classification_report(test_cat, pred_cat, \n",
    "                            target_names = [x for x in word_encoder.classes_]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
