{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_simplified', 'sample_submission.csv', 'train_raw.zip', 'test_raw.csv', 'test_simplified.csv', 'train_simplified.zip']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "STROKE_COUNT = 196\n",
    "TRAIN_SAMPLES = 1000\n",
    "VALID_SAMPLES = 750\n",
    "TEST_SAMPLES = 750\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "def top_3_accuracy(x,y): return top_k_categorical_accuracy(x,y, 3)\n",
    "from glob import glob\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"./input\"))\n",
    "base_dir = os.path.join('./', 'input')\n",
    "test_path = os.path.join(base_dir, 'test_simplified.csv')\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "97c14d114840a4157b904370868df5ebf71b734d"
   },
   "outputs": [],
   "source": [
    "import six\n",
    "def pad_sequences(sequences, maxlen=None, dtype='int32',\n",
    "                  padding='pre', truncating='pre', value=0.):\n",
    "    \"\"\"Pads sequences to the same length.\n",
    "    This function transforms a list of\n",
    "    `num_samples` sequences (lists of integers)\n",
    "    into a 2D Numpy array of shape `(num_samples, num_timesteps)`.\n",
    "    `num_timesteps` is either the `maxlen` argument if provided,\n",
    "    or the length of the longest sequence otherwise.\n",
    "    Sequences that are shorter than `num_timesteps`\n",
    "    are padded with `value` at the end.\n",
    "    Sequences longer than `num_timesteps` are truncated\n",
    "    so that they fit the desired length.\n",
    "    The position where padding or truncation happens is determined by\n",
    "    the arguments `padding` and `truncating`, respectively.\n",
    "    Pre-padding is the default.\n",
    "    # Arguments\n",
    "        sequences: List of lists, where each element is a sequence.\n",
    "        maxlen: Int, maximum length of all sequences.\n",
    "        dtype: Type of the output sequences.\n",
    "            To pad sequences with variable length strings, you can use `object`.\n",
    "        padding: String, 'pre' or 'post':\n",
    "            pad either before or after each sequence.\n",
    "        truncating: String, 'pre' or 'post':\n",
    "            remove values from sequences larger than\n",
    "            `maxlen`, either at the beginning or at the end of the sequences.\n",
    "        value: Float or String, padding value.\n",
    "    # Returns\n",
    "        x: Numpy array with shape `(len(sequences), maxlen)`\n",
    "    # Raises\n",
    "        ValueError: In case of invalid values for `truncating` or `padding`,\n",
    "            or in case of invalid shape for a `sequences` entry.\n",
    "    \"\"\"\n",
    "    if not hasattr(sequences, '__len__'):\n",
    "        raise ValueError('`sequences` must be iterable.')\n",
    "    lengths = []\n",
    "    for x in sequences:\n",
    "        if not hasattr(x, '__len__'):\n",
    "            raise ValueError('`sequences` must be a list of iterables. '\n",
    "                             'Found non-iterable: ' + str(x))\n",
    "        lengths.append(len(x))\n",
    "\n",
    "    num_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "\n",
    "    is_dtype_str = np.issubdtype(dtype, np.str_) or np.issubdtype(dtype, np.unicode_)\n",
    "    if isinstance(value, six.string_types) and dtype != object and not is_dtype_str:\n",
    "        raise ValueError(\"`dtype` {} is not compatible with `value`'s type: {}\\n\"\n",
    "                         \"You should set `dtype=object` for variable length strings.\"\n",
    "                         .format(dtype, type(value)))\n",
    "\n",
    "    x = np.full((num_samples, maxlen) + sample_shape, value, dtype=dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if not len(s):\n",
    "            continue  # empty list/array was found\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" '\n",
    "                             'not understood' % truncating)\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError('Shape of sample %s of sequence at position %s '\n",
    "                             'is different from expected shape %s' %\n",
    "                             (trunc.shape[1:], idx, sample_shape))\n",
    "\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "    return x\n",
    "\n",
    "from ast import literal_eval\n",
    "ALL_TRAIN_PATHS = glob(os.path.join(base_dir, 'train_simplified', '*.csv'))\n",
    "COL_NAMES = ['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word']\n",
    "\n",
    "def _stack_it(raw_strokes):\n",
    "    \"\"\"preprocess the string and make \n",
    "    a standard Nx3 stroke vector\"\"\"\n",
    "    stroke_vec = literal_eval(raw_strokes) # string->list\n",
    "    # unwrap the list\n",
    "    in_strokes = [(xi,yi,i)  \n",
    "     for i,(x,y) in enumerate(stroke_vec) \n",
    "     for xi,yi in zip(x,y)]\n",
    "    c_strokes = np.stack(in_strokes)\n",
    "    # replace stroke id with 1 for continue, 2 for new\n",
    "    c_strokes[:,2] = [1]+np.diff(c_strokes[:,2]).tolist()\n",
    "    c_strokes[:,2] += 1 # since 0 is no stroke\n",
    "    # pad the strokes with zeros\n",
    "    # print(c_strokes)\n",
    "    return pad_sequences(c_strokes.swapaxes(0, 1), \n",
    "                         maxlen=STROKE_COUNT, \n",
    "                         padding='post').swapaxes(0, 1)\n",
    "def read_batch(samples=5, start_row = 0):\n",
    "    \"\"\"\n",
    "    load and process the csv files\n",
    "    this function is horribly inefficient but simple\n",
    "    \"\"\"\n",
    "    out_df_list = []\n",
    "    for c_path in ALL_TRAIN_PATHS:\n",
    "        c_df = pd.read_csv(c_path, nrows=samples, skiprows=start_row)\n",
    "        #print(type(c_df), \";\")\n",
    "        c_df.columns=COL_NAMES\n",
    "        out_df_list += [c_df[['drawing', 'word']]]\n",
    "        #print(out_df_list, \"==\")\n",
    "    full_df = pd.concat(out_df_list)\n",
    "    full_df['drawing'] = full_df['drawing'].\\\n",
    "        map(_stack_it)\n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "639d4e09764ef362bd6273dd6e95504fa33501c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set is loading\n",
      "training set is loaded\n",
      "dev set is loaded\n",
      "test set is loaded\n",
      "words 340 => The Eiffel Tower, The Great Wall of China, The Mona Lisa, airplane, alarm clock, ambulance, angel, animal migration, ant, anvil, apple, arm, asparagus, axe, backpack, banana, bandage, barn, baseball, baseball bat, basket, basketball, bat, bathtub, beach, bear, beard, bed, bee, belt, bench, bicycle, binoculars, bird, birthday cake, blackberry, blueberry, book, boomerang, bottlecap, bowtie, bracelet, brain, bread, bridge, broccoli, broom, bucket, bulldozer, bus, bush, butterfly, cactus, cake, calculator, calendar, camel, camera, camouflage, campfire, candle, cannon, canoe, car, carrot, castle, cat, ceiling fan, cell phone, cello, chair, chandelier, church, circle, clarinet, clock, cloud, coffee cup, compass, computer, cookie, cooler, couch, cow, crab, crayon, crocodile, crown, cruise ship, cup, diamond, dishwasher, diving board, dog, dolphin, donut, door, dragon, dresser, drill, drums, duck, dumbbell, ear, elbow, elephant, envelope, eraser, eye, eyeglasses, face, fan, feather, fence, finger, fire hydrant, fireplace, firetruck, fish, flamingo, flashlight, flip flops, floor lamp, flower, flying saucer, foot, fork, frog, frying pan, garden, garden hose, giraffe, goatee, golf club, grapes, grass, guitar, hamburger, hammer, hand, harp, hat, headphones, hedgehog, helicopter, helmet, hexagon, hockey puck, hockey stick, horse, hospital, hot air balloon, hot dog, hot tub, hourglass, house, house plant, hurricane, ice cream, jacket, jail, kangaroo, key, keyboard, knee, ladder, lantern, laptop, leaf, leg, light bulb, lighthouse, lightning, line, lion, lipstick, lobster, lollipop, mailbox, map, marker, matches, megaphone, mermaid, microphone, microwave, monkey, moon, mosquito, motorbike, mountain, mouse, moustache, mouth, mug, mushroom, nail, necklace, nose, ocean, octagon, octopus, onion, oven, owl, paint can, paintbrush, palm tree, panda, pants, paper clip, parachute, parrot, passport, peanut, pear, peas, pencil, penguin, piano, pickup truck, picture frame, pig, pillow, pineapple, pizza, pliers, police car, pond, pool, popsicle, postcard, potato, power outlet, purse, rabbit, raccoon, radio, rain, rainbow, rake, remote control, rhinoceros, river, roller coaster, rollerskates, sailboat, sandwich, saw, saxophone, school bus, scissors, scorpion, screwdriver, sea turtle, see saw, shark, sheep, shoe, shorts, shovel, sink, skateboard, skull, skyscraper, sleeping bag, smiley face, snail, snake, snorkel, snowflake, snowman, soccer ball, sock, speedboat, spider, spoon, spreadsheet, square, squiggle, squirrel, stairs, star, steak, stereo, stethoscope, stitches, stop sign, stove, strawberry, streetlight, string bean, submarine, suitcase, sun, swan, sweater, swing set, sword, t-shirt, table, teapot, teddy-bear, telephone, television, tennis racquet, tent, tiger, toaster, toe, toilet, tooth, toothbrush, toothpaste, tornado, tractor, traffic light, train, tree, triangle, trombone, truck, trumpet, umbrella, underwear, van, vase, violin, washing machine, watermelon, waterslide, whale, wheel, windmill, wine bottle, wine glass, wristwatch, yoga, zebra, zigzag\n"
     ]
    }
   ],
   "source": [
    "train_args = dict(samples=TRAIN_SAMPLES, \n",
    "                  start_row=0)\n",
    "valid_args = dict(samples=VALID_SAMPLES, \n",
    "                  start_row=train_args['samples']+1)\n",
    "test_args = dict(samples=TEST_SAMPLES, \n",
    "                 start_row=valid_args['samples']+train_args['samples']+1)\n",
    "print(\"training set is loading\")\n",
    "train_df = read_batch(**train_args)\n",
    "print(\"training set is loaded\")\n",
    "valid_df = read_batch(**valid_args)\n",
    "print(\"dev set is loaded\")\n",
    "test_df = read_batch(**test_args)\n",
    "print(\"test set is loaded\")\n",
    "word_encoder = LabelEncoder()\n",
    "word_encoder.fit(train_df['word'])\n",
    "print('words', len(word_encoder.classes_), '=>', ', '.join([x for x in word_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "76e0faf7712b7a73b86c20151521342f4853222b"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "#print(\"test_train_df\", test_train_df)\n",
    "#print(\"train_df\", train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "cbaa639b93602235b38616476994515d074f425b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(340000, 196, 3)\n"
     ]
    }
   ],
   "source": [
    "def to_categorical(y, num_classes=None, dtype='float32'):\n",
    "    \"\"\"Converts a class vector (integers) to binary class matrix.\n",
    "    E.g. for use with categorical_crossentropy.\n",
    "    # Arguments\n",
    "        y: class vector to be converted into a matrix\n",
    "            (integers from 0 to num_classes).\n",
    "        num_classes: total number of classes.\n",
    "        dtype: The data type expected by the input, as a string\n",
    "            (`float32`, `float64`, `int32`...)\n",
    "    # Returns\n",
    "        A binary matrix representation of the input. The classes axis\n",
    "        is placed last.\n",
    "    # Example\n",
    "    ```python\n",
    "    # Consider an array of 5 labels out of a set of 3 classes {0, 1, 2}:\n",
    "    > labels\n",
    "    array([0, 2, 1, 2, 0])\n",
    "    # `to_categorical` converts this into a matrix with as many\n",
    "    # columns as there are classes. The number of rows\n",
    "    # stays the same.\n",
    "    > to_categorical(labels)\n",
    "    array([[ 1.,  0.,  0.],\n",
    "           [ 0.,  0.,  1.],\n",
    "           [ 0.,  1.,  0.],\n",
    "           [ 0.,  0.,  1.],\n",
    "           [ 1.,  0.,  0.]], dtype=float32)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes), dtype=dtype)\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical\n",
    "def get_Xy(in_df):\n",
    "    X = np.stack(in_df['drawing'], 0)\n",
    "    y = to_categorical(word_encoder.transform(in_df['word'].values))\n",
    "    return X, y\n",
    "train_X, train_y = get_Xy(train_df)\n",
    "valid_X, valid_y = get_Xy(valid_df)\n",
    "test_X, test_y = get_Xy(test_df)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "acace200b4b1dba9b949de1db5fb5c6ef47313b1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "a4aa2a57984d00732e22dfd30b874419c5f8f37c"
   },
   "outputs": [],
   "source": [
    "class HParams():\n",
    "    def __init__(self):\n",
    "        self.Nz = 128\n",
    "        self.M = 20\n",
    "        self.ConvKernelSize = 5\n",
    "        self.dropout = 0.3\n",
    "        self.batch_size = 100\n",
    "        self.eta_min = 0.01\n",
    "        self.R = 0.99995\n",
    "        self.KL_min = 0.2\n",
    "        self.wKL = 0.5\n",
    "        self.lr = 0.001\n",
    "        self.lr_decay = 0.9999\n",
    "        self.min_lr = 0.00001\n",
    "        self.grad_clip = 1.\n",
    "        self.temperature = 0.4\n",
    "        self.max_seq_length = 200\n",
    "\n",
    "hp = HParams()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "22ae8feab9270d7fd6e570fb6744c609a98b0f52"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "a2ee19a81147231e0ea2452eeb60ec1049a3f488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "def make_batch_slice(start_index, batch_size):\n",
    "    end_index = min(train_X.shape[0], start_index+batch_size)\n",
    "    x_slice = train_X[start_index:end_index]\n",
    "    y_slice = train_y[start_index:end_index]\n",
    "    x_slice = x_slice.swapaxes(0,1)\n",
    "    batch = Variable(torch.from_numpy(np.asarray(x_slice)).float()).to(device)\n",
    "    ytags = Variable(torch.from_numpy(np.asarray(y_slice)).long()).to(device)\n",
    "    #print(\"batch\",batch)\n",
    "    return batch, ytags\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe33d3d33ba7c45dd3ad8e6309dd2ae27c2d6902"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "cad4dc71731053b830e80a0fb8d7c553911cbb3b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/cuda/__init__.py:116: UserWarning: \n",
      "    Found GPU0 GRID K520 which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "class SketchRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(SketchRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.conv1d_1 = nn.Conv1d(input_size, 48, 5)\n",
    "        self.dropout_1 = nn.Dropout(0.3)\n",
    "        self.conv1d_2 = nn.Conv1d(48, 64, 5)\n",
    "        self.dropout_2 = nn.Dropout(0.3)\n",
    "        self.conv1d_3 = nn.Conv1d(64, 96, 3)\n",
    "        self.dropout_3 = nn.Dropout(0.3)\n",
    "        self.lstm_1 = nn.LSTM(96,hidden_size, n_layers, dropout=hp.dropout,batch_first=True)\n",
    "        self.fc_mu = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        inputs = inputs.transpose(0, 1).transpose(1, 2)\n",
    "        #print(\"inputs\",type(inputs))\n",
    "        #print(\"inputs\",inputs.size())\n",
    "        output = self.conv1d_1(inputs)\n",
    "        #print(output.size())\n",
    "        output = self.dropout_1(output)\n",
    "        output = self.conv1d_2(output)\n",
    "        #print(output.size())\n",
    "        output = self.dropout_2(output)\n",
    "        output = self.conv1d_3(output)\n",
    "        #print(output.size())\n",
    "        output = self.dropout_3(output)\n",
    "        output = output.transpose(1, 2)\n",
    "        #output = output.transpose(1, 2).transpose(0, 1)\n",
    "        #print(\"output\",type(output))\n",
    "        output, (hidden,x) = self.lstm_1(output, hidden)\n",
    "        output_in_last_timestep=output[:,-1,:]\n",
    "        #print(\"output\",output.size())\n",
    "        #print(hidden.size())\n",
    "        #print(x.size())\n",
    "        #print(\"output_in_last_timestep\", output_in_last_timestep.size())\n",
    "        output = self.fc_mu(output_in_last_timestep)\n",
    "        #print(\"fc output\", output.size())\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden \n",
    "\n",
    "model = SketchRNN(3, 128, 340).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),weight_decay = 1e-5,lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "82f9ad28dbc582117f68b99be78a8496759e58d0"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (48) : no kernel image is available for execution on the device at /opt/conda/conda-bld/pytorch_1532579245307/work/aten/src/THC/THCTensorCopy.cu:206",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1ea46061f366>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Step 3. Run our forward pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtag_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Step 4. Compute the loss, gradients, and update the parameters by\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-4442ccdffb67>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, hidden)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#print(\"inputs\",type(inputs))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#print(\"inputs\",inputs.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1d_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m#print(output.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 176\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (48) : no kernel image is available for execution on the device at /opt/conda/conda-bld/pytorch_1532579245307/work/aten/src/THC/THCTensorCopy.cu:206"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 128\n",
    "print_every = 2\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    start_index = 0\n",
    "    hidden = None\n",
    "    while start_index < 10000:\n",
    "        sentence, tags = make_batch_slice(start_index, batch_size)\n",
    "        #print(\"sentence\",type(sentence))\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores, hidden = model(sentence, None)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        #print(\"tag_scores\", tag_scores)\n",
    "        #print(\"hidden\", hidden.size())\n",
    "        #print(\"tags\", tags.size())\n",
    "        loss = F.cross_entropy(tag_scores, torch.max(tags, 1)[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        start_index = start_index + batch_size\n",
    "        #print(start_index)\n",
    "    #if epoch % print_every == 0:\n",
    "    print('loss = %.4f ' % (loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c27c29b14ecddcf6b130a81181b34dba1310bc97"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2a54bb3de320939cebc49850544f1a8586881436"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d8c4d821f60b2c1e5024e79171e141163cfe02b6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f4fa0121e40805010359353bc32970682a46130"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "91c36d71c10647e0583d14c42374313c33601049"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
