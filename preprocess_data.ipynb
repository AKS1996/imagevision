{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocese the data\n",
    "* Reduce some data based on entropy value. Its aim is to remove too simple images and too many strokes images.\n",
    "* Create the validate and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dask import bag\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import ast\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code written by beluga from Kaggle  https://www.kaggle.com/gaborfodor/shuffle-csvs\n",
    "qd_names =['cup','garden hose', 'marker', 'truck', 'oven', 'cooler', 'birthday cake',\n",
    "'camouflage', 'pool', 'dog', 'bear','bird', 'The Great Wall of China','van',\n",
    "'tiger', 'bench', 'hot tub','coffee cup', 'telephone', 'mug','matches',\n",
    "'animal migration', 'lantern', 'skyscraper','keyboard','foot','monkey','sleeping bag',\n",
    "'brain', 'peanut', 'belt', 'tent','cookie', 'sweater','hot dog',\n",
    "'microwave', 'mermaid', 'donut', 'hourglass', 'bee']\n",
    "\n",
    "test_dir = './test'\n",
    "train_dir = './train'\n",
    "val_dir = './val'\n",
    "\n",
    "def f2cat(filename: str) -> str:\n",
    "    return filename.split('.')[0]\n",
    "\n",
    "class Simplified():\n",
    "    def __init__(self, input_path='./input'):\n",
    "        self.input_path = input_path\n",
    "\n",
    "    def list_all_categories(self):\n",
    "        return qd_names\n",
    "\n",
    "    def read_training_csv(self, category, nrows=None, usecols=None, drawing_transform=False):\n",
    "        df = pd.read_csv(os.path.join(self.input_path, 'train_simplified', category + '.csv'),\n",
    "                         nrows=nrows,parse_dates=['timestamp'], usecols=usecols)\n",
    "        if drawing_transform:\n",
    "            df['drawing'] = df['drawing'].apply(json.loads)\n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "s = Simplified()\n",
    "NCSVS = 100\n",
    "categories = s.list_all_categories()\n",
    "print(len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/100 [00:10<16:48, 10.19s/it]\u001b[A\n",
      "  2%|▏         | 2/100 [00:20<16:36, 10.17s/it]\u001b[A\n",
      "  3%|▎         | 3/100 [00:30<16:31, 10.22s/it]\u001b[A\n",
      "  4%|▍         | 4/100 [00:40<16:17, 10.19s/it]\u001b[A\n",
      "  5%|▌         | 5/100 [00:50<16:04, 10.16s/it]\u001b[A\n",
      "  6%|▌         | 6/100 [01:00<15:49, 10.10s/it]\u001b[A\n",
      "  7%|▋         | 7/100 [01:10<15:37, 10.08s/it]\u001b[A\n",
      "  8%|▊         | 8/100 [01:20<15:26, 10.07s/it]\u001b[A\n",
      "  9%|▉         | 9/100 [01:30<15:14, 10.05s/it]\u001b[A\n",
      " 10%|█         | 10/100 [01:41<15:06, 10.08s/it]\u001b[A\n",
      " 11%|█         | 11/100 [01:51<14:59, 10.10s/it]\u001b[A\n",
      " 12%|█▏        | 12/100 [02:01<14:46, 10.08s/it]\u001b[A\n",
      " 13%|█▎        | 13/100 [02:11<14:33, 10.04s/it]\u001b[A\n",
      " 14%|█▍        | 14/100 [02:21<14:23, 10.04s/it]\u001b[A\n",
      " 15%|█▌        | 15/100 [02:31<14:12, 10.03s/it]\u001b[A\n",
      " 16%|█▌        | 16/100 [02:41<14:00, 10.01s/it]\u001b[A\n",
      " 17%|█▋        | 17/100 [02:51<13:49, 10.00s/it]\u001b[A\n",
      " 18%|█▊        | 18/100 [03:01<13:40, 10.01s/it]\u001b[A\n",
      " 19%|█▉        | 19/100 [03:11<13:34, 10.05s/it]\u001b[A\n",
      " 20%|██        | 20/100 [03:21<13:25, 10.07s/it]\u001b[A\n",
      " 21%|██        | 21/100 [03:31<13:17, 10.10s/it]\u001b[A\n",
      " 22%|██▏       | 22/100 [03:41<13:09, 10.12s/it]\u001b[A\n",
      " 23%|██▎       | 23/100 [03:51<13:00, 10.14s/it]\u001b[A\n",
      " 24%|██▍       | 24/100 [04:02<12:54, 10.20s/it]\u001b[A\n",
      " 25%|██▌       | 25/100 [04:12<12:40, 10.14s/it]\u001b[A\n",
      " 26%|██▌       | 26/100 [04:22<12:28, 10.11s/it]\u001b[A\n",
      " 27%|██▋       | 27/100 [04:32<12:16, 10.10s/it]\u001b[A\n",
      " 28%|██▊       | 28/100 [04:42<12:06, 10.09s/it]\u001b[A\n",
      " 29%|██▉       | 29/100 [04:52<11:59, 10.13s/it]\u001b[A\n",
      " 30%|███       | 30/100 [05:02<11:49, 10.14s/it]\u001b[A\n",
      " 31%|███       | 31/100 [05:13<11:40, 10.15s/it]\u001b[A\n",
      " 32%|███▏      | 32/100 [05:23<11:27, 10.11s/it]\u001b[A\n",
      " 33%|███▎      | 33/100 [05:33<11:13, 10.06s/it]\u001b[A\n",
      " 34%|███▍      | 34/100 [05:43<11:06, 10.11s/it]\u001b[A\n",
      " 35%|███▌      | 35/100 [05:53<10:57, 10.11s/it]\u001b[A\n",
      " 36%|███▌      | 36/100 [06:03<10:52, 10.20s/it]\u001b[A\n",
      " 37%|███▋      | 37/100 [06:13<10:42, 10.20s/it]\u001b[A\n",
      " 38%|███▊      | 38/100 [06:24<10:29, 10.15s/it]\u001b[A\n",
      " 39%|███▉      | 39/100 [06:34<10:16, 10.11s/it]\u001b[A\n",
      " 40%|████      | 40/100 [06:43<10:03, 10.07s/it]\u001b[A\n",
      " 41%|████      | 41/100 [06:54<09:54, 10.07s/it]\u001b[A\n",
      " 42%|████▏     | 42/100 [07:04<09:42, 10.04s/it]\u001b[A\n",
      " 43%|████▎     | 43/100 [07:13<09:30, 10.01s/it]\u001b[A\n",
      " 44%|████▍     | 44/100 [07:24<09:21, 10.02s/it]\u001b[A\n",
      " 45%|████▌     | 45/100 [07:34<09:10, 10.02s/it]\u001b[A\n",
      " 46%|████▌     | 46/100 [07:44<09:00, 10.01s/it]\u001b[A\n",
      " 47%|████▋     | 47/100 [07:53<08:49,  9.99s/it]\u001b[A\n",
      " 48%|████▊     | 48/100 [08:04<08:40, 10.01s/it]\u001b[A\n",
      " 49%|████▉     | 49/100 [08:14<08:30, 10.00s/it]\u001b[A\n",
      " 50%|█████     | 50/100 [08:24<08:22, 10.05s/it]\u001b[A\n",
      " 51%|█████     | 51/100 [08:34<08:12, 10.06s/it]\u001b[A\n",
      " 52%|█████▏    | 52/100 [08:44<08:01, 10.02s/it]\u001b[A\n",
      " 53%|█████▎    | 53/100 [08:54<07:50, 10.01s/it]\u001b[A\n",
      " 54%|█████▍    | 54/100 [09:04<07:39, 10.00s/it]\u001b[A\n",
      " 55%|█████▌    | 55/100 [09:14<07:30, 10.00s/it]\u001b[A\n",
      " 56%|█████▌    | 56/100 [09:24<07:20, 10.00s/it]\u001b[A\n",
      " 57%|█████▋    | 57/100 [09:34<07:11, 10.04s/it]\u001b[A\n",
      " 58%|█████▊    | 58/100 [09:44<07:02, 10.05s/it]\u001b[A\n",
      " 59%|█████▉    | 59/100 [09:54<06:52, 10.05s/it]\u001b[A\n",
      " 60%|██████    | 60/100 [10:04<06:45, 10.14s/it]\u001b[A\n",
      " 61%|██████    | 61/100 [10:14<06:33, 10.10s/it]\u001b[A\n",
      " 62%|██████▏   | 62/100 [10:24<06:21, 10.04s/it]\u001b[A\n",
      " 63%|██████▎   | 63/100 [10:34<06:10, 10.01s/it]\u001b[A\n",
      " 64%|██████▍   | 64/100 [10:44<06:01, 10.03s/it]\u001b[A\n",
      " 65%|██████▌   | 65/100 [10:55<05:54, 10.12s/it]\u001b[A\n",
      " 66%|██████▌   | 66/100 [11:05<05:43, 10.10s/it]\u001b[A\n",
      " 67%|██████▋   | 67/100 [11:15<05:34, 10.13s/it]\u001b[A\n",
      " 68%|██████▊   | 68/100 [11:25<05:23, 10.12s/it]\u001b[A\n",
      " 69%|██████▉   | 69/100 [11:35<05:14, 10.15s/it]\u001b[A\n",
      " 70%|███████   | 70/100 [11:45<05:06, 10.21s/it]\u001b[A\n",
      " 71%|███████   | 71/100 [11:56<04:55, 10.18s/it]\u001b[A\n",
      " 72%|███████▏  | 72/100 [12:06<04:43, 10.12s/it]\u001b[A\n",
      " 73%|███████▎  | 73/100 [12:16<04:32, 10.08s/it]\u001b[A\n",
      " 74%|███████▍  | 74/100 [12:26<04:21, 10.06s/it]\u001b[A\n",
      " 75%|███████▌  | 75/100 [12:35<04:10, 10.02s/it]\u001b[A\n",
      " 76%|███████▌  | 76/100 [12:45<03:59, 10.00s/it]\u001b[A\n",
      " 77%|███████▋  | 77/100 [12:55<03:50, 10.00s/it]\u001b[A\n",
      " 78%|███████▊  | 78/100 [13:05<03:39,  9.98s/it]\u001b[A\n",
      " 79%|███████▉  | 79/100 [13:15<03:29,  9.96s/it]\u001b[A\n",
      " 80%|████████  | 80/100 [13:25<03:19,  9.95s/it]\u001b[A\n",
      " 81%|████████  | 81/100 [13:35<03:09,  9.97s/it]\u001b[A\n",
      " 82%|████████▏ | 82/100 [13:45<02:59,  9.97s/it]\u001b[A\n",
      " 83%|████████▎ | 83/100 [13:55<02:49,  9.95s/it]\u001b[A\n",
      " 84%|████████▍ | 84/100 [14:05<02:39,  9.94s/it]\u001b[A\n",
      " 85%|████████▌ | 85/100 [14:15<02:29,  9.97s/it]\u001b[A\n",
      " 86%|████████▌ | 86/100 [14:25<02:19,  9.98s/it]\u001b[A\n",
      " 87%|████████▋ | 87/100 [14:35<02:09,  9.99s/it]\u001b[A\n",
      " 88%|████████▊ | 88/100 [14:45<01:59,  9.99s/it]\u001b[A\n",
      " 89%|████████▉ | 89/100 [14:55<01:50, 10.02s/it]\u001b[A\n",
      " 90%|█████████ | 90/100 [15:05<01:40, 10.02s/it]\u001b[A\n",
      " 91%|█████████ | 91/100 [15:15<01:30, 10.00s/it]\u001b[A\n",
      " 92%|█████████▏| 92/100 [15:25<01:19,  9.99s/it]\u001b[A\n",
      " 93%|█████████▎| 93/100 [15:35<01:09,  9.99s/it]\u001b[A\n",
      " 94%|█████████▍| 94/100 [15:45<00:59,  9.96s/it]\u001b[A\n",
      " 95%|█████████▌| 95/100 [15:55<00:49,  9.97s/it]\u001b[A\n",
      " 96%|█████████▌| 96/100 [16:05<00:39, 10.00s/it]\u001b[A\n",
      " 97%|█████████▋| 97/100 [16:15<00:29,  9.99s/it]\u001b[A\n",
      " 98%|█████████▊| 98/100 [16:25<00:20, 10.02s/it]\u001b[A\n",
      " 99%|█████████▉| 99/100 [16:35<00:10, 10.01s/it]\u001b[A\n",
      "100%|██████████| 100/100 [16:45<00:00, 10.02s/it]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42779, 7)\n"
     ]
    }
   ],
   "source": [
    "low_threshold = 1\n",
    "upper_threshold = 0.5\n",
    "\n",
    "def entropy_it(x):\n",
    "    counts = np.bincount(x)\n",
    "    p = counts[counts > 0] / float(len(x))\n",
    "    return -np.sum(p * np.log2(p))\n",
    "\n",
    "def data_draw_cv2(raw_strokes, size=96, linewidth=6, time_color=True):\n",
    "    img = np.zeros((256, 256), np.uint8)\n",
    "    for t, stroke in enumerate(ast.literal_eval(raw_strokes)):\n",
    "        for i in range(len(stroke[0]) - 1):\n",
    "            color = 255 - min(t, 10) * 13 if time_color else 255\n",
    "            _ = cv2.line(img, (stroke[0][i], stroke[1][i]), \n",
    "                         (stroke[0][i + 1], stroke[1][i + 1]), color, linewidth)\n",
    "    if size != 256:\n",
    "        img = cv2.resize(img, (size, size))\n",
    "        \n",
    "    img = np.array(img)\n",
    "    return entropy_it(img.flatten()), img\n",
    "\n",
    "def create_dataset(recognized_only = True ):\n",
    "    for y, cat in enumerate(categories):\n",
    "        df = s.read_training_csv(cat,nrows=120000)\n",
    "        df = df[df['recognized'] == True].copy() if recognized_only else df\n",
    "        #Shuffle the data of the category.\n",
    "        print(\"Create \",y,\" category:\",cat,\" len\",len(df))\n",
    "        df['rnd'] = np.random.rand(len(df))\n",
    "        df = df.sort_values(by='rnd').drop('rnd', axis=1)\n",
    "\n",
    "        entropybag = bag.from_sequence(df.drawing.values).map(data_draw_cv2)\n",
    "        data = entropybag.compute()\n",
    "        entropy, images = zip(*data)\n",
    "        lower = np.percentile(entropy, low_threshold)\n",
    "        upper = np.percentile(entropy, 100 - upper_threshold)    \n",
    "        df['y'] = y\n",
    "\n",
    "        df['cv'] = entropy\n",
    "\n",
    "        df = df[(df['cv'] > lower) & (df['cv'] < upper)]\n",
    "        index = np.where((entropy > lower) & (entropy < upper))\n",
    "\n",
    "        images = np.array(images)\n",
    "        images = images[index]\n",
    "\n",
    "        print(\"After entropy\",len(df),len(images),min(entropy),max(entropy))\n",
    "\n",
    "        #Create test dataset, val dataset, and train dataset.\n",
    "        test_csv = df[0:512]\n",
    "        val_csv = df[512:1024]\n",
    "        df = df[1024:]\n",
    "        \n",
    "        df['cv'] = (df.key_id // 10 ** 7) % NCSVS\n",
    "        \n",
    "        if y == 0:\n",
    "            #np.save(os.path.join(test_dir,cat),images[0:512])\n",
    "            #np.save(os.path.join(val_csv,cat),images[0:512])\n",
    "            test_csv.to_csv(os.path.join(test_dir,'test_dataset.csv'),index=False)\n",
    "            val_csv.to_csv(os.path.join(val_dir,'val_dataset.csv'),index = False)\n",
    "        else:\n",
    "            test_csv.to_csv(os.path.join(test_dir,'test_dataset.csv'),mode = 'a',header = False, index=False)\n",
    "            val_csv.to_csv(os.path.join(val_dir,'val_dataset.csv'),mode = 'a',header = False, index=False)\n",
    "            \n",
    "        df.to_csv(os.path.join(train_dir,cat + \".csv\"),index=False)\n",
    "        \n",
    "        for k in range(NCSVS):\n",
    "            filename = 'train_k{}.csv'.format(k)\n",
    "            chunk = df[df.cv == k]\n",
    "            chunk = chunk.drop(['key_id'], axis=1)\n",
    "            if y == 0:\n",
    "                #np.save(os.path.join(train_dir,cat),images[1024:])\n",
    "                chunk.to_csv(os.path.join(train_dir,filename),index=False)\n",
    "            else:\n",
    "                chunk.to_csv(os.path.join(train_dir,filename),mode = 'a',header = False, index=False)\n",
    "\n",
    "#create_dataset()\n",
    "\n",
    "for k in tqdm(range(NCSVS)):\n",
    "\n",
    "    filename = os.path.join(train_dir,'train_k{}.csv'.format(k))\n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        df['rnd'] = np.random.rand(len(df))\n",
    "        df = df.sort_values(by='rnd').drop('rnd', axis=1)\n",
    "        df.to_csv(filename + '.gz', compression='gzip', index=False)\n",
    "        os.remove(filename)\n",
    "        \n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
